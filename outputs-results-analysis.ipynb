{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, sys, json, string, csv, math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('~/gpt-writing-prompts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import story_analysis.utils as kp_utils\n",
    "import story_analysis.attr_score_funcs as kp_funcs\n",
    "import story_analysis.run_for_data as kp_run_for_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: create dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data + metadata (pov info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_info = pd.read_csv('data/meta_info/human_info.csv.gzip', compression='gzip')\n",
    "m_info = pd.read_csv('data/meta_info/gpt_info.csv.gzip', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of stories by pov\n",
    "order = ['TP-M', 'TP-F', 'FP', 'SP', 'Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = h_info.groupby(['p_pov'])['s_pov'].value_counts(normalize=True).unstack()\n",
    "tmp1['group'] = 'human'\n",
    "\n",
    "tmp2 = m_info.groupby(['p_pov'])['s_pov'].value_counts(normalize=True).unstack()\n",
    "tmp2['group'] = 'gpt-3.5'\n",
    "tmp = pd.concat([tmp1, tmp2])\n",
    "\n",
    "display(tmp[order])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1\n",
    "fig, axs = plt.subplots(figsize=(4, 6), nrows=2, ncols=1, sharex=True, sharey=True)\n",
    "# h_info.groupby(['p_pov', 's_pov']).size().unstack().loc[order].plot(kind='bar', stacked=True, figsize=(7,5))\n",
    "h_info.groupby(['p_pov'])['s_pov'].value_counts(normalize=True).unstack().loc[order][order].\\\n",
    "plot(kind='bar', stacked=True, ax=axs[0])\n",
    "axs[0].set_xlabel(\"prompt PoV\", fontsize=12)\n",
    "axs[0].set_ylabel(\"proportion of stories\", fontsize=12)\n",
    "# plt.legend(title='Story PoV')\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "axs[0].get_legend().remove()\n",
    "axs[0].set_title(\"Human-written Stories\")\n",
    "\n",
    "# plt.figure()\n",
    "# h_info.groupby(['p_pov', 's_pov']).size().unstack().loc[order].plot(kind='bar', stacked=True, figsize=(7,5))\n",
    "m_info.groupby(['p_pov'])['s_pov'].value_counts(normalize=True).unstack().loc[order][order].\\\n",
    "plot(kind='bar', stacked=True, ax=axs[1])\n",
    "axs[1].set_xlabel(\"prompt PoV\", fontsize=12)\n",
    "axs[1].set_ylabel(\"proportion of stories\", fontsize=12)\n",
    "# axs[1].set_ylabel(\"\")\n",
    "axs[1].get_legend().remove()\n",
    "axs[1].set_title(\"GPT-generated Stories\")\n",
    "\n",
    "fig.legend(handles, labels, bbox_to_anchor=(0.1, 0.99), loc='lower left', ncol=3, prop={'size': 12})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_info.groupby('p_pov')['p_ind'].nunique().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_info.groupby('p_pov')['p_ind'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_info.ren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = pd.DataFrame(h_info.groupby('p_pov')['s_pov'].value_counts(normalize=True)).\\\n",
    "rename({'s_pov':'prop'}, axis='columns').reset_index().pivot(columns='s_pov', values='prop', index='p_pov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1.reset_index(inplace=True)\n",
    "tmp1['group'] = 'human'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp2 = pd.DataFrame(m_info.groupby('p_pov')['s_pov'].value_counts(normalize=True)).\\\n",
    "rename({'s_pov':'prop'}, axis='columns').reset_index().pivot(columns='s_pov', values='prop', index='p_pov')\n",
    "tmp2.reset_index(inplace=True)\n",
    "tmp2['group'] = 'gpt-3.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.concat([tmp1, tmp2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers for figure 1\n",
    "print(tmp.sort_values(by=['p_pov', 'group'], ascending=False)[['p_pov', 'group']+order].\\\n",
    "      to_latex(float_format='%.2f', index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_scores(readf):\n",
    "    sids, scores = [], []\n",
    "    \n",
    "    with open(os.path.join(readf, 'out_scores.csv'), 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            pid, sid, val = row\n",
    "            \n",
    "            sid = str(pid) + '-' + str(sid)\n",
    "            sids.append(sid)\n",
    "            scores.append(float(val))\n",
    "    return sids, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSIONS = ['valence', 'arousal', 'dominance', 'power', 'appearance', 'intellect']\n",
    "ATTR_METHODS = ['all', 'sub', 'comet']\n",
    "SCORE_METHODS = ['avg', 'axis', 'sim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ['TP-M', 'TP-F', 'FP', 'SP', 'Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_dfs = {}\n",
    "for dim in DIMENSIONS:\n",
    "    \n",
    "    human_dfs[dim] = {}\n",
    "    for am in ATTR_METHODS:\n",
    "        human_dfs[dim][am] = {}\n",
    "        for sm in SCORE_METHODS:\n",
    "            \n",
    "            readf = os.path.join('data/story_scores/human', dim, am, sm)\n",
    "            sids, scores = read_scores(readf)\n",
    "\n",
    "            df = pd.DataFrame({\n",
    "                        'dim': dim,\n",
    "                        'am': am,\n",
    "                        'sm': sm,\n",
    "                        's_id': sids,\n",
    "                        'score': scores\n",
    "                    })\n",
    "            df['writer'] = 'human'\n",
    "            \n",
    "            if (dim == 'appearance' or dim == 'intellect') and (sm == 'axis'):\n",
    "                df['score'] = pd.NA\n",
    "            \n",
    "            df = df.convert_dtypes()\n",
    "            ddf = pd.merge(h_info[['p_ind', 's_ind', 's_id', 's_pov']], df, on='s_id', how='left')\n",
    "            human_dfs[dim][am][sm] = ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_dfs = {}\n",
    "for dim in DIMENSIONS:\n",
    "   \n",
    "    gpt_dfs[dim] = {}\n",
    "    for am in ATTR_METHODS:\n",
    "        gpt_dfs[dim][am] = {}\n",
    "        for sm in SCORE_METHODS:\n",
    "            \n",
    "            readf = os.path.join('outputs/gen', dim, am, sm)\n",
    "            sids, scores = read_scores(readf)\n",
    "\n",
    "            df = pd.DataFrame({\n",
    "                        'dim': dim,\n",
    "                        'am': am,\n",
    "                        'sm': sm,\n",
    "                        's_id': sids,\n",
    "                        'score': scores\n",
    "                    })\n",
    "            df['writer'] = 'gpt-3.5'\n",
    "            \n",
    "            if (dim == 'appearance' or dim == 'intellect') and (am == 'axis'):\n",
    "                df['score'] = pd.NA\n",
    "            \n",
    "            df = df.convert_dtypes()\n",
    "            ddf = pd.merge(m_info[['p_ind', 's_ind', 's_id', 's_pov']], df, on='s_id', how='left')\n",
    "            gpt_dfs[dim][am][sm] = ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add z_scores\n",
    "for dim in DIMENSIONS:\n",
    "    \n",
    "    for am in ATTR_METHODS:\n",
    "        for sm in SCORE_METHODS:\n",
    "            hdf = human_dfs[dim][am][sm]\n",
    "            mdf = gpt_dfs[dim][am][sm]\n",
    "            \n",
    "            all_scores = hdf['score'].tolist() + mdf['score'].tolist()\n",
    "            \n",
    "            try:\n",
    "                z_mean = np.nanmean(all_scores)\n",
    "                z_std = np.nanstd(all_scores)\n",
    "\n",
    "                hdf['z_score'] = (hdf['score'] - z_mean) / z_std\n",
    "                mdf['z_score'] = (mdf['score'] - z_mean) / z_std\n",
    "            \n",
    "            except TypeError:\n",
    "                hdf['z_score'] = hdf['score']\n",
    "                mdf['z_score'] = mdf['score']\n",
    "            \n",
    "            human_dfs[dim][am][sm] = hdf\n",
    "            gpt_dfs[dim][am][sm] = mdf\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set axis to be best method for all bidimensional scales, sim otherwise\n",
    "best_sm_map = {x: 'axis' for x in DIMENSIONS}\n",
    "\n",
    "best_sm_map['appearance'] = 'sim'\n",
    "best_sm_map['intellect'] = 'sim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use comet attributes\n",
    "best_attr = 'comet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_order = ['TP-M', 'TP-F', 'FP', 'Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall story z-scores (not in paper)\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 2, ncols = 3, figsize=(15, 8))\n",
    "i = 0\n",
    "j = 0\n",
    "for dim in DIMENSIONS:\n",
    "    h_scores = human_dfs[dim]['all']['avg']['z_score'].tolist()\n",
    "    m_scores = gpt_dfs[dim]['all']['avg']['z_score'].tolist()\n",
    "    \n",
    "    axs[i][j].hist(h_scores, label='human', alpha=0.5)\n",
    "    axs[i][j].hist(m_scores, label='gpt-3.5', alpha=0.5)\n",
    "    axs[i][j].set_title(dim)\n",
    "    axs[i][j].legend()\n",
    "    \n",
    "    j += 1\n",
    "    if j == 3:\n",
    "        i += 1\n",
    "        j = 0\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows = 1, ncols = 3, figsize=(12, 4), sharey=True)\n",
    "i = 0\n",
    "for dim in DIMENSIONS[:3]:\n",
    "    h_scores = human_dfs[dim]['all']['avg']['score'].tolist()\n",
    "    m_scores = gpt_dfs[dim]['all']['avg']['score'].tolist()\n",
    "    \n",
    "    axs[i].hist(h_scores, label='human', alpha=0.5)\n",
    "    axs[i].hist(m_scores, label='gpt-3.5', alpha=0.5)\n",
    "    axs[i].set_title(dim.title())\n",
    "    axs[i].legend()\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall absolute story scores (not in paper)\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 3, ncols = 2, figsize=(6, 8), sharex=True)\n",
    "i = 0\n",
    "j = 0\n",
    "for dim in DIMENSIONS:\n",
    "    hdf = human_dfs[dim]['all']['avg']#['z_score'].tolist()\n",
    "    mdf = gpt_dfs[dim]['all']['avg']#['z_score'].tolist()\n",
    "    \n",
    "    bdf = pd.concat([hdf, mdf])\n",
    "    bdf.dropna(subset=['s_pov', 'score'])\n",
    "    \n",
    "    sns.pointplot(data=bdf, x='s_pov', y='score', hue='writer', join=False, ax=axs[i][j], order=keep_order)\n",
    "    handles, labels = axs[i][j].get_legend_handles_labels()\n",
    "    axs[i][j].get_legend().remove()\n",
    "    axs[i][j].grid(which='both')\n",
    "\n",
    "    axs[i][j].set_title(dim.title(), fontsize=15)\n",
    "    if j == 0:\n",
    "        axs[i][j].set_ylabel(\"score\", fontsize=15)\n",
    "    else:\n",
    "\n",
    "    axs[i][j].set_xlabel(\"\")\n",
    "    axs[i][j].xaxis.set_tick_params(labelsize=15)\n",
    "\n",
    "    j += 1\n",
    "    if j == 2:\n",
    "        i += 1\n",
    "        j = 0\n",
    "fig.legend(handles, labels, bbox_to_anchor=(0.3, 0.99), loc='lower left', ncol=2, prop={'size': 12})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall story z-scores (not in paper)\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 3, ncols = 2, figsize=(6, 8), sharex=True)\n",
    "i = 0\n",
    "j = 0\n",
    "for dim in DIMENSIONS:\n",
    "    hdf = human_dfs[dim]['all']['avg']#['z_score'].tolist()\n",
    "    mdf = gpt_dfs[dim]['all']['avg']#['z_score'].tolist()\n",
    "    \n",
    "    bdf = pd.concat([hdf, mdf])\n",
    "    bdf.dropna(subset=['s_pov', 'z_score'])\n",
    "    \n",
    "    sns.pointplot(data=bdf, x='s_pov', y='z_score', hue='writer', join=False, ax=axs[i][j], order=keep_order)\n",
    "    handles, labels = axs[i][j].get_legend_handles_labels()\n",
    "    axs[i][j].get_legend().remove()\n",
    "    axs[i][j].grid(which='both')\n",
    "\n",
    "    axs[i][j].set_title(dim.title(), fontsize=15)\n",
    "    if j == 0:\n",
    "        axs[i][j].set_ylabel(\"z_score\", fontsize=15)\n",
    "    else:\n",
    "        axs[i][j].set_ylabel(\"\")\n",
    "#     if i == 2:\n",
    "# #         axs[i][j].set_xlabel(\"story PoV\", fontsize=15)\n",
    "#     else:\n",
    "    axs[i][j].set_xlabel(\"\")\n",
    "    axs[i][j].xaxis.set_tick_params(labelsize=15)\n",
    "#     axs[i][j].legend()\n",
    "    \n",
    "    j += 1\n",
    "    if j == 2:\n",
    "        i += 1\n",
    "        j = 0\n",
    "fig.legend(handles, labels, bbox_to_anchor=(0.3, 0.99), loc='lower left', ncol=2, prop={'size': 12})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protagonist attribute z-scores (not in paper)\n",
    "fig, axs = plt.subplots(nrows = 3, ncols = 2, figsize=(6, 8), sharex=True)\n",
    "i = 0\n",
    "j = 0\n",
    "for dim in DIMENSIONS:\n",
    "    hdf = human_dfs[dim][best_attr][best_sm_map[dim]]#['z_score'].tolist()\n",
    "    mdf = gpt_dfs[dim][best_attr][best_sm_map[dim]]#['z_score'].tolist()\n",
    "    \n",
    "    bdf = pd.concat([hdf, mdf])\n",
    "    bdf.dropna(subset=['s_pov', 'z_score'])\n",
    "    \n",
    "    sns.pointplot(data=bdf, x='s_pov', y='z_score', hue='writer', join=False, ax=axs[i][j], order=keep_order)\n",
    "    handles, labels = axs[i][j].get_legend_handles_labels()\n",
    "    axs[i][j].get_legend().remove()\n",
    "    axs[i][j].grid(which='both')\n",
    "    axs[i][j].set_title(dim.title(), fontsize=15)\n",
    "    if j == 0:\n",
    "        axs[i][j].set_ylabel(\"z_score\", fontsize=15)\n",
    "    else:\n",
    "        axs[i][j].set_ylabel(\"\")\n",
    "\n",
    "    axs[i][j].set_xlabel(\"\")\n",
    "    axs[i][j].xaxis.set_tick_params(labelsize=15)\n",
    "    \n",
    "    j += 1\n",
    "    if j == 2:\n",
    "        i += 1\n",
    "        j = 0\n",
    "fig.legend(handles, labels, bbox_to_anchor=(0.3, 0.99), loc='lower left', ncol=2, prop={'size': 12})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations between avg and best_sm_method\n",
    "rows = []\n",
    "for dim in DIMENSIONS:\n",
    "    for am in ATTR_METHODS:\n",
    "        scores_1 = human_dfs[dim][am]['avg']['score'].tolist()\n",
    "        scores_2 = human_dfs[dim][am][best_sm_map[dim]]['score'].tolist()\n",
    "        \n",
    "        corr = stats.spearmanr(scores_1, scores_2, nan_policy='omit')\n",
    "        \n",
    "        rows.append([dim, am, corr[0], corr[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = pd.DataFrame(rows, columns=['dim', 'attr_method', 'sp_corr', 'sp_pval'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlations between all, sim, comet \n",
    "# rows = []\n",
    "for dim in DIMENSIONS[-2:]:\n",
    "    for sm in SCORE_METHODS:\n",
    "        if sm  == 'axis':\n",
    "            continue\n",
    "        scores_1 = human_dfs[dim]['all'][sm]['z_score'].tolist()\n",
    "        scores_2 = human_dfs[dim]['sub'][sm]['z_score'].tolist()\n",
    "        scores_3 = human_dfs[dim]['comet'][sm]['score'].tolist()\n",
    "        \n",
    "        corr1 = stats.spearmanr(scores_1, scores_2, nan_policy='omit')\n",
    "        corr2 = stats.spearmanr(scores_1, scores_3, nan_policy='omit')\n",
    "        corr3 = stats.spearmanr(scores_2, scores_3, nan_policy='omit')\n",
    "        \n",
    "        rows.append([dim, sm, corr1[0], corr1[1], corr2[0], corr2[1],corr3[0], corr3[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_corr_df = pd.DataFrame(rows, columns=['dim', 'am', 'all-sub-c', 'all-sub-p', 'all-comet-c', 'all-comet-p',\\\n",
    "                                       'sub-comet-c', 'sub-comet-p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Significance tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human vs gpt\n",
    "rows = []\n",
    "for dim in DIMENSIONS:\n",
    "    for am in ATTR_METHODS:\n",
    "        hdf = human_dfs[dim][am][best_sm_map[dim]]\n",
    "        mdf = gpt_dfs[dim][am][best_sm_map[dim]]\n",
    "        \n",
    "        hscores = hdf['z_score'].dropna().tolist()\n",
    "        mscores = mdf['z_score'].dropna().tolist()\n",
    "        \n",
    "        tt, pval = stats.ttest_ind(hscores, mscores)\n",
    "        \n",
    "        rows.append([dim, am, tt, pval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matched\n",
    "# human vs gpt\n",
    "rows = []\n",
    "for dim in DIMENSIONS:\n",
    "    for am in ATTR_METHODS:\n",
    "        hdf = human_dfs[dim][am][best_sm_map[dim]]\n",
    "        mdf = gpt_dfs[dim][am][best_sm_map[dim]]\n",
    "        \n",
    "        hagg = hdf.groupby('p_ind')['z_score'].agg(np.nanmean).reset_index().dropna(subset=['z_score'])\n",
    "        magg = mdf.groupby('p_ind')['z_score'].agg(np.nanmean).reset_index().dropna(subset=['z_score'])\n",
    "        \n",
    "        merged = hagg.merge(magg, on='p_ind', how='inner')\n",
    "        \n",
    "        hscores = merged['z_score_x'].dropna().tolist()\n",
    "        mscores = merged['z_score_y'].dropna().tolist()\n",
    "        \n",
    "        tt, pval = stats.ttest_rel(hscores, mscores)\n",
    "        \n",
    "        rows.append([dim, am, tt, pval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# male vs female\n",
    "rows = []\n",
    "for dim in DIMENSIONS:\n",
    "    for am in ATTR_METHODS:\n",
    "        hdf = human_dfs[dim][am][best_sm_map[dim]]\n",
    "#         mdf = gpt_dfs[dim][am][best_sm_map[dim]]\n",
    "        \n",
    "       \n",
    "        mscores = hdf[hdf['s_pov']=='TP-M']['z_score'].dropna().tolist()\n",
    "        fscores = hdf[hdf['s_pov']=='TP-F']['z_score'].dropna().tolist()\n",
    "        \n",
    "        tt, pval = stats.ttest_ind(fscores, mscores)\n",
    "        \n",
    "        rows.append([dim, am, tt, pval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# male vs female\n",
    "rows = []\n",
    "for dim in DIMENSIONS:\n",
    "    for am in ATTR_METHODS:\n",
    "        hdf = gpt_dfs[dim][am][best_sm_map[dim]]\n",
    "        \n",
    "       \n",
    "        mscores = hdf[hdf['s_pov']=='TP-M']['z_score'].dropna().tolist()\n",
    "        fscores = hdf[hdf['s_pov']=='TP-F']['z_score'].dropna().tolist()\n",
    "        \n",
    "        tt, pval = stats.ttest_ind(fscores, mscores)\n",
    "        \n",
    "        rows.append([dim, am, tt, pval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## protagonist groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(plt.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2\n",
    "fig, axs = plt.subplots(figsize=(12, 6), nrows=2, ncols=3, sharey=True, sharex=True)\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "for dim in DIMENSIONS:\n",
    "    # comet, best\n",
    "    hdf = human_dfs[dim]['comet'][best_sm_map[dim]]\n",
    "    mdf = gpt_dfs[dim]['comet'][best_sm_map[dim]]\n",
    "    \n",
    "    bdf = pd.concat([hdf, mdf])\n",
    "    \n",
    "    bdf.dropna(subset=['s_pov', 'z_score'], inplace=True)\n",
    "    \n",
    "    sns.pointplot(data=bdf, x='s_pov', y='z_score', hue='writer', order = keep_order,\n",
    "                join=False,  markers=\"x\", ax=axs[i][j],markersize=20, scale=2.0, errorbar='ci')\n",
    "    \n",
    "    \n",
    "    # sub, best\n",
    "    hdf = human_dfs[dim]['sub'][best_sm_map[dim]]\n",
    "    mdf = gpt_dfs[dim]['sub'][best_sm_map[dim]]\n",
    "    \n",
    "    bdf = pd.concat([hdf, mdf])\n",
    "    \n",
    "    bdf.dropna(subset=['s_pov', 'z_score'], inplace=True)\n",
    "    \n",
    "    sns.pointplot(data=bdf, x='s_pov', y='z_score', hue='writer', order = keep_order,\n",
    "                join=False, markers=\"x\", markersize=17, ax=axs[i][j], \\\n",
    "                scale=1.5, alpha=0.5, palette='pastel')\n",
    "\n",
    "    axs[i][j].set_title(dim.title(), fontsize=18)\n",
    "    if i == 1 and j == 1:\n",
    "        axs[i][j].set_xlabel(\"story PoV\", fontsize=18)\n",
    "    \n",
    "    else:\n",
    "        axs[i][j].set_xlabel(\"\")\n",
    "    \n",
    "    if j == 0:\n",
    "        axs[i][j].set_ylabel(\"score\", fontsize=18)\n",
    "        axs[i][j].tick_params(axis='y', labelsize=15)\n",
    "    else:\n",
    "        axs[i][j].set_ylabel(\"\")\n",
    "        \n",
    "    if i == 1:\n",
    "        axs[i][j].tick_params(axis='x', labelsize=15)\n",
    "\n",
    "    handles, labels = axs[i][j].get_legend_handles_labels()\n",
    "    axs[i][j].get_legend().remove()\n",
    "    axs[i][j].grid(which='both')\n",
    "    j+=1\n",
    "    if j==3:\n",
    "        i += 1\n",
    "        j = 0\n",
    "    \n",
    "\n",
    "\n",
    "new_labels = [labels[0] + '-comet', labels[1]+'-comet', labels[2]+'-spacy', labels[3]+'-spacy']\n",
    "fig.legend(handles, new_labels, bbox_to_anchor=(.5, 1.), loc='lower center', ncol=2, prop={'size': 14})\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ['TP-M', 'TP-F', 'FP', 'Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table 3\n",
    "rows = []\n",
    "for dim in DIMENSIONS:\n",
    "    hdf = human_dfs[dim][best_attr][best_sm_map[dim]]\n",
    "    mdf = gpt_dfs[dim][best_attr][best_sm_map[dim]]\n",
    "    \n",
    "    h_means = hdf.groupby('s_pov')['z_score'].agg(np.nanmean)\n",
    "    m_means = mdf.groupby('s_pov')['z_score'].agg(np.nanmean)\n",
    "    \n",
    "    h_stds = hdf.groupby('s_pov')['z_score'].agg(np.nanstd)\n",
    "    m_stds = mdf.groupby('s_pov')['z_score'].agg(np.nanstd)\n",
    "    \n",
    "    h_scores = [\"{:.2f} ({:.2f})\".format(h_means[o], h_stds[o]) for o in order]\n",
    "    m_scores = [\"{:.2f} ({:.2f})\".format(m_means[o], m_stds[o]) for o in order]\n",
    "    \n",
    "    rows.append([dim, 'human'] + h_scores)\n",
    "    rows.append([dim, 'gpt-3.5'] + m_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(rows, columns=['Dim', 'Writer'] + order).to_latex(float_format='%.3f', index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table 3 but for sub attributes (not in paper)\n",
    "rows = []\n",
    "for dim in DIMENSIONS:\n",
    "    hdf = human_dfs[dim]['sub'][best_sm_map[dim]]\n",
    "    mdf = gpt_dfs[dim]['sub'][best_sm_map[dim]]\n",
    "    \n",
    "    h_means = hdf.groupby('s_pov')['z_score'].agg(np.nanmean)\n",
    "    m_means = mdf.groupby('s_pov')['z_score'].agg(np.nanmean)\n",
    "    \n",
    "    h_stds = hdf.groupby('s_pov')['z_score'].agg(np.nanstd)\n",
    "    m_stds = mdf.groupby('s_pov')['z_score'].agg(np.nanstd)\n",
    "    \n",
    "    h_scores = [\"{:.2f} ({:.2f})\".format(h_means[o], h_stds[o]) for o in order]\n",
    "    m_scores = [\"{:.2f} ({:.2f})\".format(m_means[o], m_stds[o]) for o in order]\n",
    "    \n",
    "    rows.append([dim, 'human'] + h_scores)\n",
    "    rows.append([dim, 'gpt-3.5'] + m_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(rows, columns=['Dim', 'Writer'] + order).to_latex(float_format='%.2f', index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt-wise diffs (Figure 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 2 human stories for each prompt 5 times, average mean and std\n",
    "# Match by prompt AND pov label\n",
    "def get_pov_prompt_scores(odf, n_samples=2, n_trials=5):\n",
    "    \n",
    "    df = odf.dropna(subset=['p_ind', 's_pov', 'z_score'])\n",
    "    \n",
    "    pov2p2scores = {}\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        pov = row['s_pov']\n",
    "        pid = row['p_ind']\n",
    "        score = row['z_score']\n",
    "        if pov not in pov2p2scores:\n",
    "            pov2p2scores[pov] = defaultdict(list)\n",
    "        \n",
    "        pov2p2scores[pov][pid].append(score)\n",
    "    \n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    for pov in pov2p2scores:\n",
    "        for pid in pov2p2scores[pov]:\n",
    "            all_scores = pov2p2scores[pov][pid]\n",
    "            if len(all_scores) < n_samples:\n",
    "                continue\n",
    "                \n",
    "            means = []\n",
    "            stds = []\n",
    "            \n",
    "            for nt in range(n_trials):\n",
    "                try:\n",
    "                    s_s = random.sample(all_scores, n_samples)\n",
    "                    means.append(np.nanmean(s_s))\n",
    "                    stds.append(np.nanstd(s_s))\n",
    "                except ValueError:\n",
    "                    means.append(np.nanmean(s_s))\n",
    "                    stds.append(np.nanstd(s_s))\n",
    "            rows.append([pov, pid, np.nanmean(means), np.nanmean(stds)])\n",
    "    return pd.DataFrame(rows, columns=['pov', 'pid', 'mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_f = {\n",
    "    'mean_x': 'human_mean',\n",
    "    'std_x': 'human_std',\n",
    "    'mean_y': 'gpt_mean',\n",
    "    'std_y': 'gpt_std'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_dfs = []\n",
    "for dim in DIMENSIONS:\n",
    "    h_score_df = get_pov_prompt_scores(human_dfs[dim][best_attr][best_sm_map[dim]])\n",
    "    m_score_df = get_pov_prompt_scores(gpt_dfs[dim][best_attr][best_sm_map[dim]])\n",
    "    \n",
    "    h_score_df.dropna(inplace=True)\n",
    "    m_score_df.dropna(inplace=True)\n",
    "    \n",
    "    matched_df = h_score_df.merge(m_score_df, on=['pov', 'pid'], how='inner')\n",
    "    matched_df['dim'] = dim\n",
    "    matched_df.rename(rename_f, axis='columns', inplace=True)\n",
    "    \n",
    "    matched_dfs.append(matched_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matched_pov_pid = pd.concat(matched_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_matched_pov_pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_matched_pov_pid.to_csv('stats/matched_pov_pid_mean_std.csv', index=False)\n",
    "\n",
    "# all_matched_pov_pid = pd.read_csv('stats/matched_pov_pid_mean_std.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matched_pid = h_score_df.merge(m_score_df, on = 'pid', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 2 human stories for each prompt 5 times, average mean and std\n",
    "# match by prompt only, ignore PoV\n",
    "def get_prompt_scores(odf, n_samples=2, n_trials=5):\n",
    "    \n",
    "    df = odf.dropna(subset=['p_ind', 'z_score'])\n",
    "    \n",
    "    p2scores = {}\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        pid = row['p_ind']\n",
    "        score = row['z_score']\n",
    "        if pid not in p2scores:\n",
    "            p2scores[pid] = []\n",
    "        \n",
    "        p2scores[pid].append(score)\n",
    "        \n",
    "    rows = []\n",
    "    \n",
    "    for pid in p2scores:\n",
    "        all_scores = p2scores[pid]\n",
    "        if len(all_scores) < n_samples:\n",
    "            continue\n",
    "\n",
    "        means = []\n",
    "        stds = []\n",
    "\n",
    "        for nt in range(n_trials):\n",
    "            try:\n",
    "                s_s = random.sample(all_scores, n_samples)\n",
    "                means.append(np.nanmean(s_s))\n",
    "                stds.append(np.nanstd(s_s))\n",
    "            except ValueError:\n",
    "                means.append(np.nanmean(s_s))\n",
    "                stds.append(np.nanstd(s_s))\n",
    "        rows.append([pid, np.nanmean(means), np.nanmean(stds)])\n",
    "            \n",
    "    return pd.DataFrame(rows, columns=['pid', 'mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_dfs = []\n",
    "for dim in DIMENSIONS:\n",
    "    h_score_df = get_prompt_scores(human_dfs[dim][best_attr][best_sm_map[dim]])\n",
    "    m_score_df = get_prompt_scores(gpt_dfs[dim][best_attr][best_sm_map[dim]])\n",
    "    \n",
    "    h_score_df.dropna(inplace=True)\n",
    "    m_score_df.dropna(inplace=True)\n",
    "    \n",
    "    matched_df = h_score_df.merge(m_score_df, on='pid', how='inner')\n",
    "    matched_df['dim'] = dim\n",
    "    matched_df.rename(rename_f, axis='columns', inplace=True)\n",
    "    \n",
    "    matched_dfs.append(matched_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matched_pid = pd.concat(matched_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matched_dfs[-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_matched_pid.to_csv('stats/matched_pid_mean_std.csv', index=False)\n",
    "\n",
    "# all_matched_pid = pd.read_csv('stats/matched_pid_mean_std.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matched_pov_pid.groupby(['dim', 'pov'])['mean_diff'].agg((np.nanmean, np.nanstd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matched_pid.groupby('dim')['mean_diff'].agg((np.nanmean, np.nanstd))\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### boxplot of diffs (Fig 3a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_matched_pov_pid['mean_diff'] = all_matched_pov_pid['human_mean'] - all_matched_pov_pid['gpt_mean']\n",
    "# all_matched_pov_pid['std_diff'] = all_matched_pov_pid['human_std'] - all_matched_pov_pid['gpt_std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows = 2, ncols = 3, figsize=(20, 8))\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "for dim in DIMENSIONS:\n",
    "    \n",
    "    ddf = all_matched_pov_pid[all_matched_pov_pid['dim']==dim]\n",
    "    sns.boxplot(data=ddf, x='mean_diff', y='pov', ax = axs[i][j])\n",
    "    axs[i][j].set_title(dim.title())\n",
    "    \n",
    "    axs[i][j].grid(which='both')\n",
    "    j+=1\n",
    "    if j==3:\n",
    "        i += 1\n",
    "        j = 0\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 4))\n",
    "sns.boxplot(data = all_matched_pid, x='mean_diff', y='dim')\n",
    "plt.ylabel(\"Dimension\", fontsize=14)\n",
    "plt.xlabel(\"Score Difference by Prompt\", fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig 3b: human control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split human into two groups\n",
    "def split_into_groups(hdf):\n",
    "    hdf = hdf.dropna(subset=['z_score'])\n",
    "    p2ids = defaultdict(list)\n",
    "    for x, y in zip(hdf['p_ind'], hdf['s_id']):\n",
    "        p2ids[x].append(y)\n",
    "    \n",
    "    splits_1 = []\n",
    "    splits_2 = []\n",
    "    for pid, sids in p2ids.items():\n",
    "        if len(sids)>=4:\n",
    "            random.shuffle(sids)\n",
    "            s = len(sids)//2\n",
    "            splits_1.extend(sids[:s])\n",
    "            splits_2.extend(sids[s:])\n",
    "            \n",
    "    hdf1 = hdf[hdf['s_id'].isin(splits_1)]\n",
    "    hdf2 = hdf[hdf['s_id'].isin(splits_2)]\n",
    "    \n",
    "    return hdf1, hdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for trial in range(5):\n",
    "    for dim in DIMENSIONS:\n",
    "        hdf = human_dfs[dim]['comet'][best_sm_map[dim]]\n",
    "        hdf1, hdf2 = split_into_groups(hdf)\n",
    "\n",
    "        mdf = gpt_dfs[dim]['comet'][best_sm_map[dim]].dropna(subset=['z_score'])\n",
    "\n",
    "        hagg1 = hdf1.groupby(['p_ind'])['z_score'].agg(np.nanmean).reset_index()\n",
    "        hagg2 = hdf2.groupby(['p_ind'])['z_score'].agg(np.nanmean).reset_index()\n",
    "\n",
    "        magg = mdf.groupby(['p_ind'])['z_score'].agg(np.nanmean).reset_index()\n",
    "\n",
    "        h_control_matched = hagg1.merge(hagg2, on=['p_ind'], how='inner')\n",
    "        h_control_matched['mean_diff'] = h_control_matched['z_score_x'] - h_control_matched['z_score_y']\n",
    "\n",
    "        h_m_matched = hagg1.merge(magg, on=['p_ind'], how = 'inner')\n",
    "        h_m_matched['mean_diff'] = h_m_matched['z_score_x'] - h_m_matched['z_score_y']\n",
    "\n",
    "#         plt.hist(h_control_matched['mean_diff'], label='control', alpha=0.5)\n",
    "#         plt.hist(h_m_matched['mean_diff'], label='human-gpt', alpha=0.5)\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "\n",
    "        control_mean = np.nanmean(h_control_matched['mean_diff'])\n",
    "        hm_mean = np.nanmean(h_m_matched['mean_diff'])\n",
    "\n",
    "        rows.append([trial, dim, 'control', control_mean])\n",
    "        rows.append([trial, dim, 'gpt-3.5', hm_mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df = pd.DataFrame(rows, columns=['trial', 'dim', 'type', 'diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "sns.pointplot(data=diff_df, x = 'diff', y='dim', hue='type', markers='x', join=False, palette={'control': 'red', 'gpt-3.5': 'blue'})\n",
    "# sns.pointplot(data=diff_df, x = 'diff', y='dim', markers='X', join=False)\n",
    "plt.grid(which='both')\n",
    "plt.xlabel(\"Mean Difference\", fontsize=14)\n",
    "plt.ylabel(\"\", fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.legend(loc='best')\n",
    "plt.yticks(fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
