{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from scipy.stats import ttest_ind\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_empprompt_index():\n",
    "    previous_prompt = \"You 're at a parade where armed war veterans wave from a passing military float . Suddenly , a shrill noise rings out , and the vets simultaneously fire their weapons into the crowds .\"\n",
    "    gen_story_path = \"/ssd005/projects/story_bias/human_story_prompt/gen_story_mapping.json\"\n",
    "    with open(gen_story_path, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "    keys_list = list(json_data.keys())\n",
    "    index_prev = keys_list.index(previous_prompt)\n",
    "    return_index = int(index_prev) + 1\n",
    "    return return_index\n",
    "\n",
    "EMPTY_INDEX = get_empprompt_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(scorepath, remove_empty):\n",
    "    scorepath_new = scorepath + \"/out_scores.csv\"\n",
    "    scores_df = pd.read_csv(scorepath_new, header=None)\n",
    "    scores_df.columns = [\"pind\", \"sind\", \"score\"]\n",
    "    if remove_empty:\n",
    "        scores_df = scores_df[scores_df['pind'] != EMPTY_INDEX]\n",
    "    data = scores_df.dropna(subset=['score'])\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_all_data(emotions, attributes, scores, human_flag, remove_empty=True, return_df=False):\n",
    "    data_storage = {}\n",
    "    if human_flag:\n",
    "        root_directory='/ssd005/projects/story_bias/human_story_prompt/outputs/kp'\n",
    "    else:\n",
    "        root_directory='/ssd005/projects/story_bias/human_story_prompt/gen_outputs/kp'\n",
    "    for emotion_name in emotions:\n",
    "        emotion_path = os.path.join(root_directory, emotion_name)\n",
    "        if os.path.isdir(emotion_path):\n",
    "            for attribute_name in attributes:\n",
    "                attribute_path = os.path.join(emotion_path, attribute_name)\n",
    "                if os.path.isdir(attribute_path):\n",
    "                    for score_name in scores:\n",
    "                        score_path = os.path.join(attribute_path, score_name)\n",
    "                        if os.path.isdir(score_path):\n",
    "                            result = get_data(score_path, remove_empty)\n",
    "                            # keyname = human_key + score_path.split('kp/')[-1]\n",
    "                            keyname = score_path.split('kp/')[-1]\n",
    "                            if return_df:\n",
    "                                data_storage[keyname] = result\n",
    "                            else:\n",
    "                                data_storage[keyname] = result['score']\n",
    "    return data_storage\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['arousal','valence','intellect', 'appearance','power', 'dominance']\n",
    "attributes=[\"all\", \"full\", \"comet\", \"sub\"]\n",
    "scores=['avg','sim', 'axis']\n",
    "human_data = get_all_data(emotions, attributes, scores, human_flag=True, remove_empty=True, return_df = False)\n",
    "generated_data = get_all_data(emotions, attributes, scores, human_flag=False, remove_empty=True, return_df = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['arousal/all/avg', 'arousal/all/sim', 'arousal/all/axis', 'arousal/full/avg', 'arousal/full/sim', 'arousal/full/axis', 'arousal/comet/avg', 'arousal/comet/sim', 'arousal/comet/axis', 'arousal/sub/avg', 'arousal/sub/sim', 'arousal/sub/axis', 'valence/all/avg', 'valence/all/sim', 'valence/all/axis', 'valence/full/avg', 'valence/full/sim', 'valence/full/axis', 'valence/comet/avg', 'valence/comet/sim', 'valence/comet/axis', 'valence/sub/avg', 'valence/sub/sim', 'valence/sub/axis', 'intellect/all/avg', 'intellect/all/sim', 'intellect/full/avg', 'intellect/full/sim', 'intellect/comet/avg', 'intellect/comet/sim', 'intellect/comet/axis', 'intellect/sub/avg', 'intellect/sub/sim', 'appearance/all/avg', 'appearance/all/sim', 'appearance/full/avg', 'appearance/full/sim', 'appearance/comet/avg', 'appearance/comet/sim', 'appearance/comet/axis', 'appearance/sub/avg', 'appearance/sub/sim', 'power/all/avg', 'power/all/sim', 'power/all/axis', 'power/full/avg', 'power/full/sim', 'power/full/axis', 'power/comet/avg', 'power/comet/sim', 'power/comet/axis', 'power/sub/avg', 'power/sub/sim', 'power/sub/axis', 'dominance/all/avg', 'dominance/all/sim', 'dominance/all/axis', 'dominance/full/avg', 'dominance/full/sim', 'dominance/full/axis', 'dominance/comet/avg', 'dominance/comet/sim', 'dominance/comet/axis', 'dominance/sub/avg', 'dominance/sub/sim', 'dominance/sub/axis'])\n",
      "166572\n",
      "171033\n"
     ]
    }
   ],
   "source": [
    "print(human_data.keys())\n",
    "print(len(human_data['arousal/comet/sim']))\n",
    "print(len(generated_data['arousal/comet/sim']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_summary_stats(result_series):\n",
    "    res_lst = list(result_series)\n",
    "    mean_value = np.mean(res_lst)\n",
    "    median_value = np.median(res_lst)\n",
    "    std_deviation = np.std(res_lst)\n",
    "    return [mean_value, median_value, std_deviation]\n",
    "\n",
    "def human_gpt_summary_stats(human_data, generated_data, emotion, embed_type, score_type):\n",
    "    # embed_type = comet, sub (spacy clause based)\n",
    "    # score_type = axis, sim\n",
    "    key = emotion + \"/\" + embed_type + \"/\" + score_type\n",
    "    human_val = human_data[key]\n",
    "    gen_val = generated_data[key]\n",
    "    human_stats = single_summary_stats(human_val)\n",
    "    gen_stats = single_summary_stats(gen_val)\n",
    "    return (human_stats, gen_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabulate_summary_stats(human_data, generated_data, embed_type, score_type):\n",
    "    emotions = ['arousal','valence','intellect', 'appearance','power', 'dominance']\n",
    "    mean_lst = []\n",
    "    median_lst = []\n",
    "    stdev_lst = []\n",
    "    for emotion in emotions:\n",
    "        (human_stats, gen_stats) = human_gpt_summary_stats(human_data, generated_data, emotion, embed_type, score_type)\n",
    "        human_mean, human_median, human_std = human_stats\n",
    "        gen_mean, gen_median, gen_std = gen_stats\n",
    "        \n",
    "        mean_lst.append([emotion, f\"{human_mean:.3f}\", f\"{gen_mean:.3f}\"])\n",
    "        median_lst.append([emotion, f\"{human_median:.3f}\", f\"{gen_median:.3f}\"])\n",
    "        stdev_lst.append([emotion, f\"{human_std:.3f}\", f\"{gen_std:.3f}\"])\n",
    "\n",
    "    headers_mean = [\"Attribute\", \"Human Mean\", \"Generated Mean\"]\n",
    "    headers_median = [\"Attribute\", \"Human Median\", \"Generated Median\"]\n",
    "    headers_stdev = [\"Attribute\", \"Human Standard Deviation\", \"Generated Standard Deviation\"]\n",
    "    print(tabulate(mean_lst, headers=headers_mean, tablefmt=\"grid\"))\n",
    "    print(tabulate(median_lst, headers=headers_median, tablefmt=\"grid\"))\n",
    "    print(tabulate(stdev_lst, headers=headers_stdev, tablefmt=\"grid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+------------------+\n",
      "| Attribute   |   Human Mean |   Generated Mean |\n",
      "+=============+==============+==================+\n",
      "| arousal     |        0.087 |            0.081 |\n",
      "+-------------+--------------+------------------+\n",
      "| valence     |        0.082 |            0.082 |\n",
      "+-------------+--------------+------------------+\n",
      "| intellect   |        0.095 |            0.091 |\n",
      "+-------------+--------------+------------------+\n",
      "| appearance  |        0.089 |            0.085 |\n",
      "+-------------+--------------+------------------+\n",
      "| power       |        0.102 |            0.098 |\n",
      "+-------------+--------------+------------------+\n",
      "| dominance   |        0.079 |            0.079 |\n",
      "+-------------+--------------+------------------+\n",
      "+-------------+----------------+--------------------+\n",
      "| Attribute   |   Human Median |   Generated Median |\n",
      "+=============+================+====================+\n",
      "| arousal     |          0.086 |              0.08  |\n",
      "+-------------+----------------+--------------------+\n",
      "| valence     |          0.082 |              0.082 |\n",
      "+-------------+----------------+--------------------+\n",
      "| intellect   |          0.094 |              0.09  |\n",
      "+-------------+----------------+--------------------+\n",
      "| appearance  |          0.089 |              0.084 |\n",
      "+-------------+----------------+--------------------+\n",
      "| power       |          0.102 |              0.097 |\n",
      "+-------------+----------------+--------------------+\n",
      "| dominance   |          0.079 |              0.079 |\n",
      "+-------------+----------------+--------------------+\n",
      "+-------------+----------------------------+--------------------------------+\n",
      "| Attribute   |   Human Standard Deviation |   Generated Standard Deviation |\n",
      "+=============+============================+================================+\n",
      "| arousal     |                      0.009 |                          0.007 |\n",
      "+-------------+----------------------------+--------------------------------+\n",
      "| valence     |                      0.005 |                          0.005 |\n",
      "+-------------+----------------------------+--------------------------------+\n",
      "| intellect   |                      0.008 |                          0.006 |\n",
      "+-------------+----------------------------+--------------------------------+\n",
      "| appearance  |                      0.012 |                          0.011 |\n",
      "+-------------+----------------------------+--------------------------------+\n",
      "| power       |                      0.009 |                          0.008 |\n",
      "+-------------+----------------------------+--------------------------------+\n",
      "| dominance   |                      0.004 |                          0.003 |\n",
      "+-------------+----------------------------+--------------------------------+\n"
     ]
    }
   ],
   "source": [
    "tabulate_summary_stats(human_data, generated_data, \"comet\", \"sim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest_gen_v_human(human_data, generated_data, num_samples, sample_size, random_seed):\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    keys_list = list(human_data.keys())\n",
    "    table_data = []\n",
    "    for key in keys_list:\n",
    "        if \"sim\" not in key or \"comet\" not in key:\n",
    "            continue\n",
    "        t_stat, p_value = ttest_ind(human_data[key], generated_data[key])\n",
    "        p_samples = []\n",
    "        for _ in range(num_samples):\n",
    "            # Sample from each dataframe\n",
    "            human_sample = np.random.choice(human_data[key], size=sample_size, replace=False)\n",
    "            generated_sample = np.random.choice(generated_data[key], size=sample_size, replace=False)\n",
    "\n",
    "            # Perform t-test for each pair of samples\n",
    "            t_statistic_fm, p_value_sample = ttest_ind(human_sample, generated_sample)\n",
    "            p_samples.append(p_value_sample)\n",
    "\n",
    "        # Apply multiple comparison correction (e.g., Bonferroni correction)\n",
    "        adjusted_p_values_sample = multipletests(p_samples, method='bonferroni')[1]\n",
    "\n",
    "        average_adjusted_p_values = np.mean(adjusted_p_values_sample, axis=0)\n",
    "\n",
    "        if np.isnan(p_value):\n",
    "            continue\n",
    "        row = [key, p_value, average_adjusted_p_values]\n",
    "        table_data.append(row)\n",
    "    \n",
    "\n",
    "    # Adjust the layout and display the figure\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    custom_headers = [\"Dimension\", \"Population t-test p-value\", \"Sample t-test p-value\"]\n",
    "\n",
    "    print(tabulate(table_data, headers=custom_headers, tablefmt=\"grid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----------------------------+-------------------------+\n",
      "| Dimension            |   Population t-test p-value |   Sample t-test p-value |\n",
      "+======================+=============================+=========================+\n",
      "| arousal/comet/sim    |                 0           |            1.18072e-190 |\n",
      "+----------------------+-----------------------------+-------------------------+\n",
      "| valence/comet/sim    |                 2.31702e-06 |            1            |\n",
      "+----------------------+-----------------------------+-------------------------+\n",
      "| intellect/comet/sim  |                 0           |            3.51014e-100 |\n",
      "+----------------------+-----------------------------+-------------------------+\n",
      "| appearance/comet/sim |                 0           |            2.04207e-51  |\n",
      "+----------------------+-----------------------------+-------------------------+\n",
      "| power/comet/sim      |                 0           |            4.56721e-76  |\n",
      "+----------------------+-----------------------------+-------------------------+\n",
      "| dominance/comet/sim  |                 1.51302e-26 |            0.591262     |\n",
      "+----------------------+-----------------------------+-------------------------+\n"
     ]
    }
   ],
   "source": [
    "ttest_gen_v_human(human_data, generated_data, num_samples=5, sample_size=3000, random_seed = 999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohen's D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohen_d(group1, group2):\n",
    "    mean1, mean2 = np.mean(group1), np.mean(group2)\n",
    "    std1, std2 = np.std(group1, ddof=1), np.std(group2, ddof=1)\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "\n",
    "    pooled_std = np.sqrt(((n1 - 1) * std1**2 + (n2 - 1) * std2**2) / (n1 + n2 - 2))\n",
    "\n",
    "    cohens_d = (mean1 - mean2) / pooled_std\n",
    "\n",
    "    return cohens_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohend_gpt_v_human(human_data, generated_data):\n",
    "\n",
    "    keys_list = list(human_data.keys())\n",
    "    table_data = []\n",
    "    for key in keys_list:\n",
    "        if \"sim\" not in key or \"comet\" not in key:\n",
    "            continue\n",
    "        effect_size = cohen_d(human_data[key], generated_data[key])\n",
    "        row = [key, effect_size]\n",
    "        table_data.append(row)\n",
    "\n",
    "    custom_headers = [\"Dimension\", \"Cohen's D Effect Size\"]\n",
    "\n",
    "    print(tabulate(table_data, headers=custom_headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------------------+\n",
      "| Dimension            |   Cohen's D Effect Size |\n",
      "+======================+=========================+\n",
      "| arousal/comet/sim    |               0.804571  |\n",
      "+----------------------+-------------------------+\n",
      "| valence/comet/sim    |               0.0162609 |\n",
      "+----------------------+-------------------------+\n",
      "| intellect/comet/sim  |               0.586079  |\n",
      "+----------------------+-------------------------+\n",
      "| appearance/comet/sim |               0.419092  |\n",
      "+----------------------+-------------------------+\n",
      "| power/comet/sim      |               0.54078   |\n",
      "+----------------------+-------------------------+\n",
      "| dominance/comet/sim  |              -0.0367103 |\n",
      "+----------------------+-------------------------+\n"
     ]
    }
   ],
   "source": [
    "cohend_gpt_v_human(human_data, generated_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "38env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
